
@inproceedings{Ali2017,
  title = {Generalized {{Pseudolikelihood Methods}} for {{Inverse Covariance Estimation}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ali, Alnur and Khare, Kshitij and Oh, Sang-Yun and Rajaratnam, Bala},
  editor = {Singh, Aarti and Zhu, Jerry},
  year = {2017},
  month = apr,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {54},
  pages = {280--288},
  publisher = {{PMLR}},
  address = {{Fort Lauderdale, FL, USA}},
  url = {http://papers.nips.cc/paper/5576-optimization-methods-for-sparse-pseudo-likelihood-graphical-model-selection.pdf},
  abstract = {We introduce PseudoNet, a new pseudolikelihood-based estimator of the inverse covariance matrix, that has a number of useful statistical and computational properties. We show, through detailed experiments with synthetic and also real-world finance as well as wind power data, that PseudoNet outperforms related methods in terms of estimation error and support recovery, making it well-suited for use in a downstream application, where obtaining low estimation error can be important. We also show, under regularity conditions, that PseudoNet is consistent. Our proof assumes the existence of accurate estimates of the diagonal entries of the underlying inverse covariance matrix; we additionally provide a two-step method to obtain these estimates, even in a high-dimensional setting, going beyond the proofs for related methods. Unlike other pseudolikelihood-based methods, we also show that PseudoNet does not saturate, i.e., in high dimensions, there is no hard limit on the number of nonzero entries in the PseudoNet estimate. We present a fast algorithm as well as screening rules that make computing the PseudoNet estimate over a range of tuning parameters tractable.},
  software = {https://bitbucket.org/sangoh/gconcordopt/src/default/},
  keywords = {undirected graphical model},
  file = {G\:\\My Drive\\references\\Conference Paper\\Ali2017 - Generalized Pseudolikelihood Methods for Inverse Covariance Estimation.pdf;G\:\\My Drive\\references\\Conference Paper\\Ali2017 - Generalized Pseudolikelihood Methods for Inverse Covariance Estimation2.pdf}
}

@inproceedings{Bhimji2017,
  title = {Exploring {{Raw HEP Data}} Using {{Deep Neural Networks}} at {{NERSC}}},
  booktitle = {Proceedings of 38th {{International Conference}} on {{High Energy Physics}} \textemdash{} {{PoS}}({{ICHEP2016}})},
  author = {Bhimji, Wahid and Racah, Evan and Ko, Seyoon and Sadowski, Peter and Tull, Craig and Oh, Sang-Yun and {Prabhat}},
  year = {2017},
  month = feb,
  publisher = {{Sissa Medialab}},
  doi = {10.22323/1.282.0889},
  abstract = {High Energy Physics has made use of artificial neural networks for some time. Recently, however, there has been considerable development outside the HEP community, particularly in deep neural networks for the purposes of image recognition. We describe the deep-learning infrastructure at NERSC, and analyses built on top of this. These are capable of revealing meaningful physical content by transforming the raw data from particle physics experiments into learned high-level representations using deep convolutional neural networks (CNNs), including in unsupervised modes where no input physics knowledge or training data is used. Here we describe in detail a project for the Daya Bay Neutrino Experiment showing both unsupervised learning and how supervised convolutional deep neural networks can provide an effective classification filter with significantly better accuracy than other machine learning methods. These approaches have significant applications for use in other experiments triggers, data quality monitoring or physics analyses.},
  file = {G\:\\My Drive\\references\\Conference Paper\\Bhimji2017 - Exploring Raw HEP Data using Deep Neural Networks at NERSC.pdf}
}

@inproceedings{Cisneros-Velarde2020,
  title = {Distributionally {{Robust Formulation}} and {{Model Selection}} for the {{Graphical Lasso}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {{Cisneros-Velarde}, Pedro and Petersen, Alexander and Oh, Sang-Yun},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  year = {2020},
  month = aug,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {108},
  pages = {756--765},
  publisher = {{PMLR}},
  address = {{Online}},
  abstract = {Building on a recent framework for distributionally robust optimization, we consider inverse covariance matrix estimation for multivariate data. A novel notion of Wasserstein ambiguity set is provided that is specifically tailored to this problem, leading to a tractable class of regularized estimators. Penalized likelihood estimators for Gaussian data, specifically the graphical lasso estimator, are special cases. Consequently, a direction connection is made between the radius of the Wasserstein ambiguity and the regularization parameter, so that the level of robustness of the estimator is shown to correspond to the level of confidence with which the ambiguity set contains a distribution with the population covariance. A unique feature of the formulation is that the radius can be expressed in closed-form as a function of the ordinary sample covariance matrix. Taking advantage of this finding, a simple algorithm is developed to determine a regularization parameter for graphical lasso, using only the bootstrapped sample covariance matrices, rendering computationally expensive repeated evaluation of the graphical lasso algorithm unnecessary. Alternatively, the distributionally robust formulation can also quantify the robustness of the corresponding estimator if one uses an off-the-shelf method such as cross-validation. Finally, a numerical study is performed to analyze the robustness of the proposed method relative to other automated tuning procedures used in practice.},
  file = {G\:\\My Drive\\references\\Conference Paper\\Cisneros-Velarde2020 - Distributionally Robust Formulation and Model Selection for the Graphical Lasso.pdf}
}

@article{Hanany2000,
  title = {{{MAXIMA}}-1: A {{Measurement}} of the {{Cosmic Microwave Background Anisotropy}} on {{Angular Scales}} of 10 Arcminutes to 5 Degrees},
  author = {Hanany, S. and Ade, P. and Balbi, A. and Bock, J. and Borrill, J. and Boscaleri, A. and de Bernardis, P. and Ferreira, P. G. and Hristov, V. V. and Jaffe, A. H. and Lange, A. E. and Lee, A. T. and Mauskopf, P. D. and Netterfield, C. B. and Oh, S. and Pascale, E. and Rabii, B. and Richards, P. L. and Smoot, G. F. and Stompor, R. and Winant, C. D. and Wu, J. H. P.},
  year = {2000},
  month = dec,
  journal = {The Astrophysical Journal},
  volume = {545},
  number = {1},
  pages = {L5--L9},
  publisher = {{IOP Publishing}},
  doi = {10.1086/317322},
  file = {G\:\\My Drive\\references\\Journal Article\\Hanany2000 - MAXIMA-1.pdf}
}

@article{Jaffe2001,
  title = {Cosmology from {{MAXIMA}}-1, {{BOOMERANG}}, and {{COBE DMR Cosmic Microwave Background Observations}}},
  author = {Jaffe, A. H. and Ade, P. A. R. and Balbi, A. and Bock, J. J. and Bond, J. R. and Borrill, J. and Boscaleri, A. and Coble, K. and Crill, B. P. and de Bernardis, P. and Farese, P. and Ferreira, P. G. and Ganga, K. and Giacometti, M. and Hanany, S. and Hivon, E. and Hristov, V. V. and Iacoangeli, A. and Lange, A. E. and Lee, A. T. and Martinis, L. and Masi, S. and Mauskopf, P. D. and Melchiorri, A. and Montroy, T. and Netterfield, C. B. and Oh, S. and Pascale, E. and Piacentini, F. and Pogosyan, D. and Prunet, S. and Rabii, B. and Rao, S. and Richards, P. L. and Romeo, G. and Ruhl, J. E. and Scaramuzzi, F. and Sforna, D. and Smoot, G. F. and Stompor, R. and Winant, C. D. and Wu, J. H. P.},
  year = {2001},
  month = apr,
  journal = {Physical Review Letters},
  volume = {86},
  number = {16},
  pages = {3475--3479},
  publisher = {{American Physical Society (APS)}},
  doi = {10.1103/physrevlett.86.3475},
  file = {G\:\\My Drive\\references\\Journal Article\\Jaffe2001 - Cosmology from MAXIMA-1, BOOMERANG, and COBE DMR Cosmic Microwave Background.pdf}
}

@article{Khare2014,
  title = {A Convex Pseudolikelihood Framework for High Dimensional Partial Correlation Estimation with Convergence Guarantees},
  author = {Khare, Kshitij and Oh, Sang-Yun and Rajaratnam, Bala},
  year = {2014},
  month = sep,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {77},
  number = {4},
  pages = {803--825},
  publisher = {{Wiley}},
  doi = {10.1111/rssb.12088},
  abstract = {Sparse high dimensional graphical model selection is a topic of much interest in modern day statistics. A popular approach is to apply l1-penalties to either parametric likelihoods, or regularized regression/pseudolikelihoods, with the latter having the distinct advantage that they do not explicitly assume Gaussianity. As none of the popular methods proposed for solving pseudolikelihood-based objective functions have provable convergence guarantees, it is not clear whether corresponding estimators exist or are even computable, or if they actually yield correct partial correlation graphs. We propose a new pseudolikelihood-based graphical model selection method that aims to overcome some of the shortcomings of current methods, but at the same time retain all their respective strengths. In particular, we introduce a novel framework that leads to a convex formulation of the partial covariance regression graph problem, resulting in an objective function comprised of quadratic forms. The objective is then optimized via a co-ordinatewise approach. The specific functional form of the objective function facilitates rigorous convergence analysis leading to convergence guarantees; an important property that cannot be established by using standard results, when the dimension is larger than the sample size, as is often the case in high dimensional applications. These convergence guarantees ensure that estimators are well defined under very general conditions and are always computable. In addition, the approach yields estimators that have good large sample properties and also respect symmetry. Furthermore, application to simulated and real data, timing comparisons and numerical convergence is demonstrated. We also present a novel unifying framework that places all graphical pseudolikelihood methods as special cases of a more general formulation, leading to important insights.},
  keywords = {undirected graphical model},
  file = {G\:\\My Drive\\references\\Journal Article\\Khare2014 - A convex pseudolikelihood framework for high dimensional partial correlation.pdf}
}

@article{Khare2019,
  title = {A Scalable Sparse {{Cholesky}} Based Approach for Learning High-Dimensional Covariance Matrices in Ordered Data},
  author = {Khare, Kshitij and Oh, Sang-Yun and Rahman, Syed and Rajaratnam, Bala},
  year = {2019},
  month = jun,
  journal = {Machine Learning},
  volume = {108},
  number = {12},
  pages = {2061--2086},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10994-019-05810-5},
  keywords = {undirected graphical model},
  file = {G\:\\My Drive\\references\\Journal Article\\Khare2019 - A scalable sparse Cholesky based approach for learning high-dimensional.pdf;G\:\\My Drive\\references\\Journal Article\\Khare2019 - A scalable sparse Cholesky based approach for learning high-dimensional2.pdf}
}

@inproceedings{Koanantakool2016,
  title = {Communication-{{Avoiding Parallel Sparse}}-{{Dense Matrix}}-{{Matrix Multiplication}}},
  booktitle = {2016 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Koanantakool, Penporn and Azad, Ariful and Buluc, Aydin and Morozov, Dmitriy and Oh, Sang-Yun and Oliker, Leonid and Yelick, Katherine},
  year = {2016},
  month = may,
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/ipdps.2016.117},
  abstract = {Multiplication of a sparse matrix with a dense matrix is a building block of an increasing number of applications in many areas such as machine learning and graph algorithms. However, most previous work on parallel matrix multiplication considered only both dense or both sparse matrix operands. This paper analyzes the communication lower bounds and compares the communication costs of various classic parallel algorithms in the context of sparse-dense matrix-matrix multiplication. We also present new communication-avoiding algorithms based on a 1D decomposition, called 1.5D, which - while suboptimal in dense-dense and sparse-sparse cases - outperform the 2D and 3D variants both theoretically and in practice for sparse-dense multiplication. Our analysis separates one-time costs from per iteration costs in an iterative machine learning context. Experiments demonstrate speedups up to 100x over a baseline 3D SUMMA implementation and show parallel scaling over 10 thousand cores.},
  file = {G\:\\My Drive\\references\\Conference Paper\\Koanantakool2016 - Communication-Avoiding Parallel Sparse-Dense Matrix-Matrix Multiplication.pdf}
}

@inproceedings{Koanantakool2018,
  title = {Communication-{{Avoiding Optimization Methods}} for {{Distributed Massive}}-{{Scale Sparse Inverse Covariance Estimation}}},
  booktitle = {Proceedings of the {{Twenty}}-{{First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Koanantakool, Penporn and Ali, Alnur and Azad, Ariful and Buluc, Aydin and Morozov, Dmitriy and Oliker, Leonid and Yelick, Katherine and Oh, Sang-Yun},
  editor = {Storkey, Amos and {Perez-Cruz}, Fernando},
  year = {2018},
  month = apr,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {84},
  pages = {1376--1386},
  publisher = {{PMLR}},
  address = {{Playa Blanca, Lanzarote, Canary Islands}},
  abstract = {Across a variety of scientific disciplines, sparse inverse covariance estimation is a popular tool for capturing the underlying dependency relationships in multivariate data. Unfortunately, most estimators are not scalable enough to handle the sizes of modern high-dimensional data sets (often on the order of terabytes), and assume Gaussian samples. To address these deficiencies, we introduce HP-CONCORD, a highly scalable optimization method for estimating a sparse inverse covariance matrix based on a regularized pseudolikelihood framework, without assuming Gaussianity. Our parallel proximal gradient method uses a novel communication-avoiding linear algebra algorithm and runs across a multi-node cluster with up to 1k nodes (24k cores), achieving parallel scalability on problems with up to {$\approx$}819 billion parameters (1.28 million dimensions); even on a single node, HP-CONCORD demonstrates scalability, outperforming a state-of-the-art method. We also use HP-CONCORD to estimate the underlying dependency structure of the brain from fMRI data, and use the result to identify functional regions automatically. The results show good agreement with a clustering from the neuroscience literature.},
  file = {G\:\\My Drive\\references\\Conference Paper\\Koanantakool2018 - Communication-Avoiding Optimization Methods for Distributed Massive-Scale.pdf}
}

@article{Levinson2011,
  title = {Copy {{Number Variants}} in {{Schizophrenia}}: Confirmation of {{Five Previous Findings}} and {{New Evidence}} for 3q29 {{Microdeletions}} and {{VIPR2 Duplications}}},
  author = {Levinson, Douglas F. and Duan, Jubao and Oh, Sang and Wang, Kai and Sanders, Alan R. and Shi, Jianxin and Zhang, Nancy and Mowry, Bryan J. and Olincy, Ann and Amin, Farooq and Cloninger, C. Robert and Silverman, Jeremy M. and Buccola, Nancy G. and Byerley, William F. and Black, Donald W. and Kendler, Kenneth S. and Freedman, Robert and Dudbridge, Frank and Pe{\textbackslash}textquotesingleer, Itsik and Hakonarson, Hakon and Bergen, Sarah E. and Fanous, Ayman H. and Holmans, Peter A. and Gejman, Pablo V.},
  year = {2011},
  month = mar,
  journal = {American Journal of Psychiatry},
  volume = {168},
  number = {3},
  pages = {302--316},
  publisher = {{American Psychiatric Publishing}},
  doi = {10.1176/appi.ajp.2010.10060876},
  file = {G\:\\My Drive\\references\\Journal Article\\Levinson2011 - Copy Number Variants in Schizophrenia.pdf}
}

@article{Levinson2012,
  title = {Genome-{{Wide Association Study}} of {{Multiplex Schizophrenia Pedigrees}}},
  author = {Levinson, Douglas F. and Shi, Jianxin and Wang, Kai and Oh, Sang and Riley, Brien and Pulver, Ann E. and Wildenauer, Dieter B. and Laurent, Claudine and Mowry, Bryan J. and Gejman, Pablo V. and Owen, Michael J. and Kendler, Kenneth S. and Nestadt, Gerald and Schwab, Sibylle G. and Mallet, Jacques and Nertney, Deborah and Sanders, Alan R. and Williams, Nigel M. and Wormley, Brandon and Lasseter, Virginia K. and Albus, Margot and {Godard-Bauch{\'e}}, Stephanie and Alexander, Madeline and Duan, Jubao and O'Donovan, Michael C. and Walsh, Dermot and O'Neill, Anthony and Papadimitriou, George N. and Dikeos, Dimitris and Maier, Wolfgang and Lerer, Bernard and Campion, Dominique and Cohen, David and Jay, Maurice and Fanous, Ayman and Eichhammer, Peter and Silverman, Jeremy M. and Norton, Nadine and Zhang, Nancy and Hakonarson, Hakon and Gao, Cynthia and Citri, Ami and Hansen, Mark and Ripke, Stephan and Dudbridge, Frank and {and}, Peter A. Holmans},
  year = {2012},
  month = sep,
  journal = {American Journal of Psychiatry},
  volume = {169},
  number = {9},
  pages = {963--973},
  publisher = {{American Psychiatric Publishing}},
  doi = {10.1176/appi.ajp.2012.11091423},
  file = {G\:\\My Drive\\references\\Journal Article\\Levinson2012 - Genome-Wide Association Study of Multiplex Schizophrenia Pedigrees.pdf}
}

@incollection{Oh2014,
  title = {Optimization {{Methods}} for {{Sparse Pseudo}}-{{Likelihood Graphical Model Selection}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Oh, Sang-Yun and Dalal, Onkar and Khare, Kshitij and Rajaratnam, Bala},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {667--675},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of {$\mathscr{l}$}1 penalized estimation in the Gaussian framework. Though many of these approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses cyclic coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing {$\mathscr{l}$}1-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous pay offs for {$\mathscr{l}$}1-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.},
  keywords = {undirected graphical model},
  file = {G\:\\My Drive\\references\\Book Section\\Oh2014 - Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection.pdf}
}

@inproceedings{Racah2016,
  title = {Revealing {{Fundamental Physics}} from the {{Daya Bay Neutrino Experiment}} Using {{Deep Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Racah, Evan and Ko, Seyoon and Sadowski, Peter and Bhimji, Wahid and Tull, Craig and Oh, Sang-Yun and Baldi, Pierre and {Prabhat}},
  year = {2016},
  doi = {10.1109/icmla.2016.0160},
  abstract = {Experiments in particle physics produce enormous quantities of data that must be analyzed and interpreted by teams of physicists. This analysis is often exploratory, where scientists are unable to enumerate the possible types of signal prior to performing the experiment. Thus, tools for summarizing, clustering, visualizing and classifying high-dimensional data are essential. In this work, we show that meaningful physical content can be revealed by transforming the raw data into a learned high-level representation using deep neural networks, with measurements taken at the Daya Bay Neutrino Experiment as a case study. We further show how convolutional deep neural networks can provide an effective classification filter with greater than 97\% accuracy across different classes of physics events, significantly better than other machine learning approaches.},
  file = {G\:\\My Drive\\references\\Conference Paper\\Racah2016 - Revealing Fundamental Physics from the Daya Bay Neutrino Experiment using Deep.pdf}
}

@article{Stompor2001,
  title = {Making Maps of the Cosmic Microwave Background: The {{MAXIMA}} Example},
  author = {Stompor, Radek and Balbi, Amedeo and Borrill, Julian D. and Ferreira, Pedro G. and Hanany, Shaul and Jaffe, Andrew H. and Lee, Adrian T. and Oh, Sang and Rabii, Bahman and Richards, Paul L. and Smoot, George F. and Winant, Celeste D. and Wu, Jiun-Huei Proty},
  year = {2001},
  month = dec,
  journal = {Physical Review D},
  volume = {65},
  number = {2},
  publisher = {{American Physical Society (APS)}},
  doi = {10.1103/physrevd.65.022003},
  file = {G\:\\My Drive\\references\\Journal Article\\Stompor2001 - Making maps of the cosmic microwave background.pdf}
}

@article{Wu2001,
  title = {Asymmetric {{Beams}} in {{Cosmic Microwave Background Anisotropy Experiments}}},
  author = {Wu, J. H. P. and Balbi, A. and Borrill, J. and Ferreira, P. G. and Hanany, S. and Jaffe, A. H. and Lee, A. T. and Oh, S. and Rabii, B. and Richards, P. L. and Smoot, G. F. and Stompor, R. and Winant, C. D.},
  year = {2001},
  month = jan,
  journal = {The Astrophysical Journal Supplement Series},
  volume = {132},
  number = {1},
  pages = {1--17},
  publisher = {{IOP Publishing}},
  doi = {10.1086/318947},
  file = {G\:\\My Drive\\references\\Journal Article\\Wu2001 - Asymmetric Beams in Cosmic Microwave Background Anisotropy Experiments.pdf}
}

@article{Zapata2019,
  title = {Partial {{Separability}} and {{Functional Graphical Models}} for {{Multivariate Gaussian Processes}}},
  author = {Zapata, Javier and Oh, Sang-Yun and Petersen, Alexander},
  year = {2019},
  month = oct,
  journal = {Submitted},
  eprint = {1910.03134v2},
  eprinttype = {arxiv},
  abstract = {The covariance structure of multivariate functional data can be highly complex, especially if the multivariate dimension is large, making extension of statistical methods for standard multivariate data to the functional data setting quite challenging. For example, Gaussian graphical models have recently been extended to the setting of multivariate functional data by applying multivariate methods to the coefficients of truncated basis expansions. However, a key difficulty compared to multivariate data is that the covariance operator is compact, and thus not invertible. The methodology in this paper addresses the general problem of covariance modeling for multivariate functional data, and functional Gaussian graphical models in particular. As a first step, a new notion of separability for multivariate functional data is proposed, termed partial separability, leading to a novel Karhunen-Lo\`eve-type expansion for such data. Next, the partial separability structure is shown to be particularly useful in order to provide a well-defined Gaussian graphical model that can be identified with a sequence of finite-dimensional graphical models, each of fixed dimension. This motivates a simple and efficient estimation procedure through application of the joint graphical lasso. Empirical performance of the method for graphical model estimation is assessed through simulation and analysis of functional brain connectivity during a motor task.},
  archiveprefix = {arXiv},
  keywords = {graphical models},
  file = {G\:\\My Drive\\references\\Journal Article\\Zapata2019 - Partial Separability and Functional Graphical Models for Multivariate Gaussian.pdf}
}


